{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiO8CWKV5yxya/mN5FY/4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varun1608/Automatic-Sentence-Completion/blob/main/AutoSentenceCompletion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***STEP1: IMPORT LIBRARIES***\n"
      ],
      "metadata": {
        "id": "zq5bra4f9VZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiXtQ_yV8Itt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***STEP 2: LOAD YOUR FILE***\n"
      ],
      "metadata": {
        "id": "NUabWXDa9fWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "bomfvWxB9sgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***STEP 3:OPEN AND PRE-PROCESS THE DATA***"
      ],
      "metadata": {
        "id": "gJzpoGzf9wn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"dataset.txt\", \"r\", encoding = \"utf8\")\n",
        "\n",
        "lines = []\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "\n",
        "\n",
        "data = \"\"\n",
        "for i in lines:\n",
        "  data = ' '. join(lines)\n",
        "\n",
        "\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
        "\n",
        "\n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "data[:500]"
      ],
      "metadata": {
        "id": "2uNFeDOd94Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***STEP 4: HYPERPARAMETER TUNING***"
      ],
      "metadata": {
        "id": "9sUI41XQAM5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "def create_model(learning_rate=0.001, embedding_dim=10, lstm_units=100, dense_units=100):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=3))\n",
        "    model.add(LSTM(lstm_units))\n",
        "    model.add(Dense(dense_units, activation=\"relu\"))\n",
        "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=learning_rate))\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'embedding_dim': [10, 50, 100],\n",
        "    'lstm_units': [50, 100, 200],\n",
        "    'dense_units': [50, 100, 200]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "grid_result = grid.fit(X, y)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
      ],
      "metadata": {
        "id": "O4QgTWV-AQr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***STEP 5:IMPLEMENT TOKENIZATION AND MAKE ADDITIONAL ADJUSMENTS***"
      ],
      "metadata": {
        "id": "p02bTWMN-CBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]\n",
        "len(sequence_data)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "sequences = []\n",
        "\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "\n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "print(\"Data: \", X[:10])\n",
        "print(\"Response: \", y[:10])\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ],
      "metadata": {
        "id": "YCbokoak-LrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***STEP 6: NEURAL ARCHITECTURE SEARCH***"
      ],
      "metadata": {
        "id": "O1elGcRg-UeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "import random\n",
        "\n",
        "def create_model(learning_rate=0.001, embedding_dim=10, lstm_units=1000, dense_units=1000, num_lstm_layers=2):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=3))\n",
        "\n",
        "    for _ in range(num_lstm_layers):\n",
        "        model.add(LSTM(lstm_units, return_sequences=True))\n",
        "\n",
        "    model.add(LSTM(lstm_units))\n",
        "    model.add(Dense(dense_units, activation=\"relu\"))\n",
        "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=learning_rate))\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "param_dist = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'embedding_dim': [10, 50, 100],\n",
        "    'lstm_units': [100, 500, 1000],\n",
        "    'dense_units': [100, 500, 1000],\n",
        "    'num_lstm_layers': [1, 2, 3]\n",
        "}\n",
        "\n",
        "n_iter_search = 10\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=n_iter_search, cv=3)\n",
        "random_search_result = random_search.fit(X, y)\n",
        "\n",
        "print(\"Best: %f using %s\" % (random_search_result.best_score_, random_search_result.best_params_))\n"
      ],
      "metadata": {
        "id": "j1DSIReP-gn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***STEP 7: CREATING A MODEL***"
      ],
      "metadata": {
        "id": "hvPI9VGM_Rt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=3))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "mC9APqFGA9E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***STEP 8: PLOT THE MODEL***"
      ],
      "metadata": {
        "id": "EJbRTztABACc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "keras.utils.plot_model(model, to_file='plot.png', show_layer_names=True)"
      ],
      "metadata": {
        "id": "6tZKTavzBD5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***STEP 9: TRAIN THE MODEL***"
      ],
      "metadata": {
        "id": "ucfGf5_SBPHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
        "model.fit(X, y, epochs=70, batch_size=64, callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "0M_rDXnoBTQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***STEP 10: LET'S PREDICT***"
      ],
      "metadata": {
        "id": "R7EaoB59BZjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "model = load_model('next_words.h5')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \"\"\n",
        "\n",
        "  for key, value in tokenizer.word_index.items():\n",
        "      if value == preds:\n",
        "          predicted_word = key\n",
        "          break\n",
        "\n",
        "  print(predicted_word)\n",
        "  return predicted_word\n",
        "while(True):\n",
        "  text = input(\"Enter your line: \")\n",
        "\n",
        "  if text == \"0\":\n",
        "      print(\"Execution completed.....\")\n",
        "      break\n",
        "\n",
        "  else:\n",
        "      try:\n",
        "          text = text.split(\" \")\n",
        "          text = text[-3:]\n",
        "          print(text)\n",
        "\n",
        "          Predict_Next_Words(model, tokenizer, text)\n",
        "\n",
        "      except Exception as e:\n",
        "        print(\"Error occurred: \",e)\n",
        "        continue"
      ],
      "metadata": {
        "id": "QeLayXVsBdjW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}